{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - RAG Model for QA Bot:\n",
    "\n",
    "File System in VS code:-\n",
    "\n",
    "Main_Folder:\n",
    "    |- Document\n",
    "          |- File.pdf      # The file which content to feed data to your business Bot.\n",
    "    |- venv                # Enviroment for the model.     \n",
    "    |- Sub_folder          # The folder in which .ipynb is stored.\n",
    "          |- ChatBot.ipynb # For business Q/A.\n",
    "    |- .env                # This variable enviroment where all API keys are stored.          \n",
    "    |- requirement.txt     # A text file which use to store all uded installation libraries\n",
    "\n",
    "Used APIs list & others component\n",
    "    |- OpenAI_API_KEY \n",
    "    |- Pinecone_API_KEY\n",
    "    |- Pinecone_enviroment # To store embended vectors database\n",
    "\n",
    "This is a reuseable code by the changing only document can use for other task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries \n",
    "\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "\n",
    "def read_doc(file_path):\n",
    "    file_loader = PyPDFLoader(file_path)  # Use PyPDFLoader for single PDF files\n",
    "    documents = file_loader.load()  # Load the content of the PDF\n",
    "    return documents\n",
    "\n",
    "\n",
    "doc = read_doc(r'C:\\LangChain\\documents\\Paper_On_DropOut.pdf')\n",
    "\n",
    "# Display the content\n",
    "for page in doc:\n",
    "    print(\"Metadata:\", page.metadata)  \n",
    "    print(\"Content:\", page.page_content[:500])  \n",
    "\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the docs into chunks\n",
    "\n",
    "def chunk_data(docs, chunk_size= 1000, chunk_overlap= 50):\n",
    "    text_splitter= RecursiveCharacterTextSplitter(chunk_size= chunk_size, chunk_overlap= chunk_overlap)\n",
    "    doc= text_splitter.split_documents(docs)\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents= chunk_data(docs= doc)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Technique of OpenAI\n",
    "\n",
    "embeddings= OpenAIEmbeddings(api_key= os.environ['OpenAI_API_KEY'])\n",
    "embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates vectors\n",
    "\n",
    "vectors= embeddings.embed_query(\"How are you?\")\n",
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector searchDB in Pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key= \"Pinecone_API_KEY\",\n",
    "    enviroment= \"Enviroment_name_from_Pinecone\"\n",
    ")\n",
    "index_name= \"Index_name_from_Pinecone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index= Pinecone.from_documents(doc, embeddings, index_name= index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consine similarity to retrieve results from vectorDB\n",
    "\n",
    "def retireve_query(query, k= 2):\n",
    "    matching_result= index.similarity_search(query, k= k)\n",
    "    return matching_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm= OpenAI(model_name= \"Model_Name\", temperature= 0.5)\n",
    "chain= load_qa_chain(llm, chian_type= \"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search answers from VectorDB\n",
    "def retireve_answer(query):\n",
    "    doc_search= retireve_query(query)\n",
    "    print(doc_search)\n",
    "    response= chain.run(input_documents= doc_search, quetion= query)\n",
    "    return response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query= \"Who invented Dropout?\"\n",
    "answer= retireve_answer(user_query)\n",
    "print(answer) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
